{
 "metadata": {
  "language": "lua",
  "name": "",
  "signature": "sha256:eb6ff47344325a18aaccf6e00c9eb2405e314c968bde32eaf316a57f7a340e3f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'torch'\n",
      "require 'math'\n",
      "local loader = require 'iris_loader'\n",
      "local train = require 'train'\n",
      "\n",
      "torch.manualSeed(1)\n",
      "data = loader.load_data()\n",
      "\n",
      "opt = {\n",
      "  nonlinearity_type = 'sigmoid',\n",
      "  training_iterations = 150, -- note: the code uses *batches*, not *minibatches*, now.\n",
      "  print_every = 25,          -- how many iterations to skip between printing the loss\n",
      "}\n",
      "\n",
      "-- train sigmoid and requ versions\n",
      "model_sigmoid, losses_sigmoid = train(opt, data)\n",
      "-- TODO: uncomment once you implement requ\n",
      "opt.nonlinearity_type = 'requ'\n",
      "model_requ, losses_requ = train(opt, data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "--------------------------------\t\n",
        "Loaded. Sizes:\t\n",
        "inputs\t 150\n",
        "   4\n",
        "[torch.LongStorage of size 2]\n",
        "\n",
        "targets\t 150\n",
        "[torch.LongStorage of size 1]\n",
        "\n",
        "--------------------------------\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration   25, loss = 0.691419\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration   50, loss = 0.575468\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration   75, loss = 0.490374\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration  100, loss = 0.411512\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration  125, loss = 0.353268\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration  150, loss = 0.309571\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration   25, loss = 0.577100\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration   50, loss = 0.451337\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration   75, loss = 0.364584\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration  100, loss = 0.300108\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration  125, loss = 0.257032\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "iteration  150, loss = 0.227033\t\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-- plot\n",
      "gnuplot.pngfigure('sigmoid.png')\n",
      "gnuplot.plot({'sigmoid',\n",
      "  torch.range(1, #losses_sigmoid), -- x-coordinates\n",
      "  torch.Tensor(losses_sigmoid),    -- y-coordinates\n",
      "  '-'}\n",
      "  -- TODO: uncomment when you implement requ\n",
      "  , {'requ',\n",
      "  torch.range(1, #losses_requ),    -- x-coordinates\n",
      "  torch.Tensor(losses_requ),       -- y-coordinates\n",
      "  '-'}\n",
      ")\n",
      "gnuplot.plotflush()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"sigmoid.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "models = { \n",
      "    requ = model_requ,  -- TODO: uncomment once you implement requ\n",
      "    sigmoid = model_sigmoid \n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for model_name, model in pairs(models) do\n",
      "  -- classification error on train set\n",
      "  local log_probs = model:forward(data.inputs)\n",
      "  local _, predictions = torch.max(log_probs, 2)\n",
      "  print(string.format('# correct for %s:', model_name))\n",
      "  print(torch.mean(torch.eq(predictions:long(), data.targets:long()):double()))\n",
      "\n",
      "  -- classification region in one slice (cf. Figure 1 scatterplots in writeup)\n",
      "  -- not pretty, but the best we can do without hacking away at gnuplot or using another library\n",
      "  local f1 = 4 -- feature on first axis\n",
      "  local f2 = 3 -- feature on second axis\n",
      "  local size = 60  -- resolution\n",
      "  local f1grid = torch.linspace(data.inputs[{{},f1}]:min(), data.inputs[{{},f1}]:max(), size)\n",
      "  local f2grid = torch.linspace(data.inputs[{{},f2}]:min(), data.inputs[{{},f2}]:max(), size)\n",
      "  local result = torch.Tensor(size, size)\n",
      "  local input = data.inputs[1]:clone()\n",
      "  for i=1,size do\n",
      "    input[f1] = f1grid[i]\n",
      "    for j=1,size do\n",
      "      input[f2] = f2grid[j]\n",
      "      result[{i,j}] = math.exp(model:forward(input)[1])\n",
      "    end\n",
      "  end\n",
      "  result[1][1] = 0 -- ugly hack to get the right scale\n",
      "  result[1][2] = 1 -- ugly hack to get the right scale\n",
      "  gnuplot.pngfigure('result.png')\n",
      "  gnuplot.imagesc(result, model_name)\n",
      "  gnuplot.plotflush()\n",
      "end\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "# correct for sigmoid:\t\n",
        "0.97333333333333\t\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "# correct for requ:\t\n",
        "0.98\t\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"result.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dofile(\"gradcheck.lua\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "3.1863114947155e-05\t\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dofile(\"jacobiancheck.lua\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "-2.9718\n",
        " 1.7070\n",
        "-0.4305\n",
        "-2.2820\n",
        " 0.5237\n",
        " 0.0004\n",
        "-1.2039\n",
        " 3.5283\n",
        " 0.4434\n",
        " 0.5848\n",
        "[torch.DoubleTensor of size 10]\n",
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  3.4139 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  1.0473  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0008 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  7.0566  0.0000  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.8868  0.0000\n",
        "-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  1.1696\n",
        "[torch.DoubleTensor of size 10x10]\n",
        "\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  3.4139  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  1.0473  0.0000  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0008  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  7.0566  0.0000  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8868  0.0000\n",
        " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.1696\n",
        "[torch.DoubleTensor of size 10x10]\n",
        "\n",
        "6.7343020493816e-12\t0\t4.312914470006e-10\t\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}